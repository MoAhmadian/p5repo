%*******************************************************
\section{DBaaS Information Leakage Management}
\label{LeakagePreventionSection}
%*******************************************************
Data warehouse of DBaaS can be considered as a coarse-grained data storage to store a large number of outsourced databases. Beyond the defined relationships among some the datasets, such as primary or foreign key relationships, often, undefined hidden relations known as \emph{cross-correlation} exist among the heterogeneous data types. The analysis of cross-correlations reveals sensitive information denoted as information leakage. Two basic methods for quantifying information leakage due to existence of two types of cross-correlation are discussed in this section. Note that these methods require searching entire databases for exact match join attributes which is very time consuming and only is achievable for small size databases. Then in the next sections we present a method for very large scale databases.  
Now before going to details, first we define the \emph{attribute class} which is called in the leakage quantification method.

\begin{definition}[Attribute Class] \hfill \\
\label{def:attributesClasses}
Attributes of a document are categorized into three classes: (1) Identifier attribute, which uniquely identifies a single document in the database, e.g. social security number, phone number and email address. (2) Semi-identifier attributes which are not able to uniquely identify a document, but collectively are able to distinguish a document, e.g. combination of name, department name, gender and age may identify a person uniquely. (3) Feature attribute expresses a characteristic of an object by giving sensitive information about it.
\end{definition} 

\noindent {\bf Explicit correlation.} A common identifier attribute value, links databases based on an equality match condition. This circumstance is described as \emph{explicit correlation}. A correlation function in Equation \ref{eq:attributeCorrelation} assigns a non-negative real number value as correlation score to a each involved datasets. 

A fundamental requirement to manage the amount of information leakage,  is to introduce measurement metrics. The first metric we developed, is basically the improved version of the metric that was introduced by Whang et al.\cite{whang2010managing}. The well-known \emph{precision} and \emph{recall} concepts were adopted from data retrieval to measure the information leakage. Precision is defined as the ratio of the number of attributes in a document $d$ to the number of attributes in the reference document. In Whang et al method, an equal weight has been assigned to all the attributes, while, we improved this model by varying  the weight of attributes according to their type.\\

\noindent For accurate quantifying of information leakage, a notion $ \Omega$ is defined as a quantitative metric for representing the value of information stored in a document $d$. This metric is the summation of $0 \leq \omega_\imath \leq 1$ which is associated for any attribute $A_\imath$ in the document, obtained by $\Omega=\sum\limits_{\imath=1}^n \omega_\imath$. The higher magnitude of the corresponding $\omega_\imath$ value reflects the importance of an attribute $A_\imath=\{key, value\}$. Consequently, for any document $d$ the value of $\Omega$ determines the value of all information in the document. The highest possible value  $\omega_\imath = 1$ is assigned to the identifier attributes. The weight value for a group of $m$ semi-identifier attributes that collectively identify an entity, is assigned to be  $\omega_\imath=\frac{1}{m}$. Moreover, a feature attribute receives a very small $\omega_\imath$ value. Equation \ref{eq:weightedRecallPrecison} represents the weight calculation for any given document.\\

\begin{equation}
\label{eq:weightedRecallPrecison}
\begin{aligned}
& \Omega_d=\alpha \times n + \beta \times m + \gamma \times p \qquad
 Such~ that:~ \alpha \gg \beta \gg \gamma \geq 0
\end{aligned}
\end{equation}
Where $n$, $m$ and $p$ are the number of identifier, semi-identifier and feature attributes accordingly with assigned weight $\alpha$, $\beta$  and $\gamma$ respectively.


As a result of dynamic schema employed for data model, in the context of the NoSQL database, the documents related to the same object are allowed to have various number of attributes. However, a full list of attributes is necessary to create a reference document. Therefore, we define a logical operator $\delta$ denoted as \emph{Super Document}, that aggregates all attributes related to an entity in the scope of collection to create the reference document required for precision and recall. 

Evidently, the cloud insider has a wide view scope in terms of accessible data for $\delta$ function. Having comprehensive super document is impractical; however, it can be formed for any subset of the selected databases (two or more). To construct a super document of an entity from set of databases, first the super document is initialized with a document that describes the entity ($d_i$). Second, the other documents ($d_j$) are scanned to extract any undetected attributes of the entity, $\mathcal{L}(\delta_{\imath},d_\jmath)$. Equation \ref{eq:SuperDocument} demonstrates the notion of super document. The construction of a super document from multiple data collections is described in Algorithm \ref{algo:ExractingLeakedInformation} (refer to Appendix \ref{app:Algorithms}).

\begin{equation}
\label{eq:SuperDocument}
\begin{aligned}
\begin{cases}
&\delta_{\imath}=d_\imath \\
&\forall d_\jmath \in \mathcal{D}, \quad \delta_{\imath}= \delta_{\imath} \cup \mathcal{L}(\delta_{\imath},d_\jmath)
\end{cases}
\end{aligned}
\end{equation}


Measuring the leaked information as a result of attribute correlation with utilization of $\delta$ function (super document) and $\omega_i$ value, for precision and recall metrics is straightforward computation. The process to create $\delta$ among the multiple collections that are hosted by the same cloud DBaaS is outlined in the Algorithm \ref{algo:ExractingLeakedInformation}. Once a new attribute is extracted, it will be appended to the super document, then a back track is needed in order to check for new paths which were already closed. The search space grows exponentially according to the number of attributes.

To eliminate the cross-correlation between databases, we intentionally insert documents containing false or misleading values for sensitive attributes denoted as \emph{disinformation document} which includes common attributes shared with the original document. As a result of disinformation document insertion, the extraction of new attributes value from correlation will be more expensive for an attacker by factor of the number of disinformation per original document.

\noindent \textsc{\textbf{Example 1.}} Consider five documents from different databases selected from the DBaaS warehouse, belonging to two individuals. The goal is to extract leaked information using attribute correlation. This case exemplifies Algorithm \ref{algo:ExractingLeakedInformation} presented in Appendix \ref{app:Algorithms} and table \ref{tab:weightTable} shows the list of attributes for this example with hypothetical weight.


\begin{table*}[htp]
\caption{Weights of attributes}
\label{tab:weightTable}
\centering
\begin{tabular}{lcc}
\toprule
\textbf{Attribute} & \textbf{Type} & \textbf{$\omega$}\\
\midrule
Zip      & Semi-identifier  & 0.3 \\ 
Address  & Semi-identifier  & 0.5 \\
Phone    & identifier       & 1.0 \\
Account  & identifier       & 1.0 \\
Name     & Semi-identifier  & 0.6 \\
Age      & Feature          & 0.1 \\
Income   & Feature          & 0.1 \\
SSN      & identifier       & 1.0 \\
Email    & identifier       & 1.0 \\
\bottomrule
\end{tabular}
\end{table*}

\begin{align*}
d_1&=\{ {\bf zip} : 456 ,~{\bf address}:``2512~ Uni.~NY",~ phone:\underline{111}\}\\
d_2&=\{ {\bf ssn} : 123,~ {\bf age} : 33,~ {\bf account}: 222 \}\\
d_3&=\{ {\bf name} : ``Kate~Jones",~ {\bf age} : 30,~ {\bf address}: ``abc",{\bf email}:``kj@a.com" \}\\
d_4&=\{ {\bf name} : ``Mike~Smith", {\bf income} :70k,~ {\bf ssn}:123, ~ {\bf phone}:\underline{111}\}\\
d_5&=\{ {\bf name} : ``Kate~Jones",{\bf email}:``kj@a.com",~ {\bf ssn} : 777\}
\end{align*}


\noindent In the above example, $\delta_{Mike~Smith},~\delta_{Kate~Jones}$ are formed by utilizing Algorithm \ref{algo:ExractingLeakedInformation}. The extractable information through the correlation are as follows:
\begin{align*}
\mu(d_1,d_2)&=FALSE & \delta&=\{d_1\}& \mathcal{L}&=\{\}\\
\mu(d_1,d_3)&=FALSE & \delta&=\{d_1\}& \mathcal{L}&=\{\}\\
\mu(d_1,d_4)&=TRUE  & \delta&=\{d_1, d_4\}& \mathcal{L}&=\{name,income,ssn\}\\
&\bm{Back~Track} &&&&\\
\mu(d_1,d_2)&=TRUE  & \delta&=\{d_1, d_4, d_2\}& \mathcal{L}&=\{name,income,ssn, age, account\}\\
\mu(d_1,d_3)&=FALSE  &\delta&=\{d_1, d_4, d_2\}& \mathcal{L}&=\{name,income,ssn, age, account\}\\
\mu(d_1,d_5)&=FALSE  &\delta&=\{d_1, d_4, d_2\}& \mathcal{L}&=\{name,income,ssn, age, account\}\\
\noalign{$\delta_{Mike~Smith}$=\{ zip,address,phone,name,income,ssn,age,account\}}
\noalign{Similarly, the super document for ``Kate Jones" is : $\delta_{Kate~Jones}=\{ name, age, address, email, ssn\}$}
\end{align*}

The recall value for these documents can be calculated:\\ $R_{d_1}=\frac{1.7}{5.4} \approx 0.31$ , $R_{d_2}=\frac{2.1}{5.4} \approx 0.39$ , $R_{d_3}=\frac{2.2}{3.2} = 0.69$, $R_{d_4}=\frac{2.6}{5.4} = 0.48$ and $R_{d_5}=\frac{2.5}{3.2} = 0.78$.\\ 
In the example above, the disinformation documents ($\rho_1,.., \rho_6,$) with low precision are created as bellow. After inserting them into the original collections, the cloud insider has double values for each sensitive attributes. Therefore, the real value for attributes cannot be extracted with high confidence.\\
\begin{align*}
\rho_1&=\{ {\bf zip} : 654, {\bf address} : ``1500 Place AZ", {\bf income} : 60k, {\bf ssn}:321 , {\bf phone} : 111\}\\
\rho_2&=\{ {\bf ssn} : 321, {\bf age} : 43, {\bf address}: ``abc AZ" , {\bf phone} : 876 \}\\
\rho_3&=\{ {\bf ssn} : 321, {\bf account} : 444\}\\
\noalign{Similarly for ``Kate Jones" we have :}
\rho_4&=\{{\bf age} : 20,~ {\bf address}: ``efd",{\bf email}:``kj@a.com"\}\\
\rho_5&=\{ {\bf name} : ``Claire Shepard",~ {\bf ssn} : 543,~{\bf email}:``kj@a.com"\}\\
\rho_6&=\{ {\bf ssn} : 543,~{\bf email}:``xy@b.com"\}\\
\end{align*}

$P_{\rho_1}=\frac{1}{2.9} =	0.34$; $P_{\rho_2}=\frac{0}{2.6}=0$; 
$P_{\rho_3}=\frac{0}{2} = 0$ ; $P_{\rho_4}=\frac{1}{1.6} \approx	0.62$ ; $P_{\rho_5}=\frac{1}{2.5} \approx	0.4$; $P_{\rho_6}=0$ \\

In short, it is more desirable to have low recall and precision values which reflects more uncertainty, and consequently less information leakage. The precision and recall are suitable metrics for quantifying the exact match information leakage in the document level that are spread in the collections. Furthermore, other probabilistic means are required to measure the information leakage due to statistical properties of attributes in very large databases. Under those circumstances, we introduce our second method to measure leaked information due to statistical correlations.

\medskip


\noindent {\bf Implicit correlation.} Sometime between two data elements there might be hidden mutual dependencies which means observation of one data item could result in inferring meaningful information about the other. The mutual dependency leaks out sensitive information about secret data which was supposed to be confidential. This leakage can be even more high-risk when the data is processed in an outsourced platform such as cloud DBaaS. In other word, the relational database system benefits from explicit data field correlations as an effective feature for database normalization and improving query efficiency. However, identifying implicit and semantically correlated subset of attributes with different data types is a challenging work. We use information theoretical methods to quantify implicit correlations. To facilitate discussion, an example of statistical property correlation of attributes is given below.\\


\noindent\textbf{\textsc{Example 2.}} An on-line stock exchange website stores the information of share in the cloud DBaaS. One simple analysis indicates there are correlation between number of buyers, the quantity of buy orders and quantity of sell orders\footnote{If there are more buyers than sellers, it signifies a price increase; on the other hand, more sellers and high volume indicates a price drop. }. In the scenario of this example, the price trend is derived from the statistical properties of two different attributes.\\

\noindent The {\it mutual information} measures the amount of information that can be obtained about attribute $X$ by observing attribute $Y$. The mutual information ranges from $0$ (if two random variables are statistically independent) to $H(X)$ (if the random variables are fully dependent). Equation \ref{eq:mutualInformation} quantifies the dependency between two attributes belonging to any two selected databases which are represented by two random variables $X$ and $Y$. $I(X; Y)$ is the quantified amount of leakage.

\begin{equation} 
\label{eq:mutualInformation}
I(X; Y ) = I(Y; X)=H(X)-H(X|Y)
\end{equation} 

In this expression, $H\left(X\right)$ and $H\left(X|Y \right)$ are the entropy of $X$ marginal distributions of $X$ and $Y$. Time complexity of Algorithm \ref{algo:mutualInformation} (see Appendix \ref{app:Algorithms}) is depended on the number of documents in the database. For a pair of attributes with $n$ instances the time complexity is $O(n^2)$. Now, the statistical analysis techniques can be called frequently in the life time of the dynamic dataset to evaluate the leakage value. Evidently, there is a maximum tolerable amount of leakage value which is considered as a threshold to initiate disinformation padding. This technique improves the constant disinformation padding which introduces huge overhead for dataset. In other words, instead of having a huge number of disinformation document, we selectively insert fake documents into the dataset when the leakage value exceeds the threshold value. The proposed technique is presented in Algorithm \ref{algo:disnformation} in Appendix \ref{app:Algorithms}. The time complexity  of algorithm \ref{algo:disnformation} is $O(\binom n2)$ which simplifies to $O(n^2)$. 

\medskip

\noindent {\bf Performance cost and mitigations of disinformation.} Insertion of disinformation documents increases the size of database and it can negatively affect the query execution time. In order to quantify the query latency in cloud DBaaS, an iterative method is employed to evaluate latency of several simple queries on the different databases that contain specific number of documents. In this way, we only focus on a single variable, which is the size of the database. The benchmark initially removes all documents from all databases and repopulates those with the required dataset size. Subsequently, two different tests are performed, with and without index. Five major query classes, including equality check, comparison, logic, range and aggregate are considered for query processing benchmark shown in Figure \ref{queryLatencyToSize}. In order to eliminate cache boost-up in the tests, the query caching is disabled. This process is repeated for all of the specified database sizes and the measurement for the benchmark without using index is displayed in Figure \ref{queryLatencyToSize}a.\\


One of the biggest reward of our approach which is coming from using the unmodified standard database server, is the benefit of all database technology features such as indexing. Indexing allows to perform more sophisticated search on data, such as binary search, that reduces the maximum search space drastically from $O(n)$ to $O(log n)$ and consequently a remarkable improvement in the performance. Figure \ref{queryLatencyToSize}b presents the improvement of the same queries execution time. The chart for a simple query on the non-indexed databases demonstrates that query latency steadily increases with rise of database size. However, the trend of query processing time remains steady, and shows no significant variations with increasing the size of indexed database. The indexed attributes guarantee an insignificant change in query processing time, especially for the encrypted databases which have the augmented size in comparison with the plaintext non-indexed database.

Indexing over data elements offers fast query processing time in the database. This experiment is designed to examine indexing over 10 encrypted and diluted databases with disinformation documents. To do so we measured the database response time for the queries when the data elements are indexed. Then, we measured query processing time with indexed encrypted database. The measurement process in both cases were automated and run under the control of the designed script which collected the processing time. The results are discussed in the next subsection.

%Figure performance
\begin{figure}[H]
\begin{subfigure}{0.40\textwidth}
\centering
\resizebox{0.9\totalheight}{!}{\input{figures/LatencyVsSize.tex}}
\label{fig:LatencyVsSize}
\caption{databases without indexes }
\end{subfigure}
\qquad \qquad
\begin{subfigure}{0.40\textwidth}
\resizebox{0.9\totalheight}{!}{\input{figures/LatencyWithIndexedDB.tex}}
\label{fig:LatencyWithIndexedDB}
\caption{databases with indexes}
\end{subfigure}
\caption{The performance analysis of query processing as a function of database size. }
\label{queryLatencyToSize}
\end{figure}

\medskip

\noindent {\bf Results and discussion.} Typical Cloud database services ensure advanced availability and scalability, but the data confidentiality and integrity are yet an area of interest to be explored further. The problem of secure processing of outsourced datasets with limited leakage is investigated in this work. The sensitive data in a single dataset can be protected with using crypto-systems. However, in the cloud DBaaS platform which is a pool of thousands of datasets, the aggregation of datasets introduces a new source of information leakage. The primary goal of this work is that the untrusted DBaaS should learn minimum information from the accumulation of data belonging to the group of users. All risks associated with an untrusted cloud DBaaS is investigated and a mitigation solution is proposed. 

User applications expect to receive valid and accurate information in response of the issued queries, not the fake information. Most NoSQL databases have a different performance with processing the same query over different database size. Our experimental benchmarks demonstrate no significant variations in performance, with a linear increase in the size of database. The small performance penalty is negligible. This can be explained by the multilevel indexing which are utilized by NoSQL databases to provide a fast access time and short latency for query processing over larger databases. To overcome the second challenge, we propose and analyze an efficient algorithm based on the signature schemes to filter out the noisy documents.


