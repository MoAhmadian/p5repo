%*************************************************
\section{Conclusions and future work}
\label{sec:conclusion}
%*************************************************
For leakage prevention, insertion of disinformation documents is studied to break the links between the documents of among a group of datasets. Simultaneously, fake documents introduces two types of challenges to the systems. First, along with the data overhead growth, it causes system performance degradation. Second, the increase in communication cost between sever and the proxy which is due to transportation of the fakes documents.   

We introduced two techniques to mitigate the negative effects of data overhead. First, we present leakage quantitative methods to identify the leakage points in datasets; therefore, a selective disinformation document insertion is proposed to minimize leakage just in those points instead of the indiscriminate document insertion. Second, using multi-level indexes as one of the most efficient tools to improve query performance by maintaining unique values in a collection. With an index, the database process queries by simply scanning the index and fetching documents as they are refereed whereas without index entire table space must be searched. Thus the performance increase is substantial. 

We investigate explicit and implicit information leakage and practical metrics to evaluation and measurement are introduced. A fast sensitivity analysis method based on approximate query processing (AQP) for any dataset with multi-level sensitivity is presented. To enhance the performance and speed up of the AQP based classification we use multi-layer sampling with different sizes to provide query response with variety of acceptable error rates. Using AQP, elevates the scalability of sensitivity analysis to any database size in the interactive speed. With the proposed method, the large latency of analytical aggregate queries which severely limits the feasibility of many analysis applications will be resolved.

In addition, from both privacy and security perspective, having a limited access to the original database and instead processing the aggregate queries on very small set of samples is less risky for data owners in the cloud platform.

The process of AQP based classification using the uniform random samples provides adequate response with less computation and data access. In computer cloud, the proposed method demands fewer resources and much shorter computation time which contributes to less cost in cloud model. Ultimately, these features helps the cloud users to compute more complex aggregate computation through less expensive way.

Then, we propose a method to estimate the size of cross-correlation using biased sampling with reasonable level of inaccuracy. The optimum sample size results in subnational speed up in cross-correlation analysis with closer answer to the real value. Knowing all parameters including sensitive documents,attributes with high degree of connectivity and the cardinality of cross-correlation enables the data owners to selectively insert disinformation document only for sensitive attributes that contribute to high level of cross-correlations.

The sensitivity and cross-correlation analysis over multiple databases in cloud DbaaS only can be conducted with CSP who has access to all datasets. As future work, we suggest to have a cross-correlation monitoring service in public cloud. Form the privacy point of view it can be consider a users' privacy violation, but CSP might can define a Cross-correlation index (CCI) on small random samples of data and by using our method this index can be monitored  by CSP when its exceeds a specified amount, CSP alert the data owner. 
